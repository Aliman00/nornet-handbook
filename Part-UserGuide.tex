% ###########################################################################
% #### The NorNet Handbook                                               ####
% #### Copyright (C) 2012-2015 Thomas Dreibholz                          ####
% ###########################################################################

% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
\chapter{Basics}
\label{cha:Basics}
% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

% ###########################################################################
\section{Introduction}
\label{sec:Introduction}
% ###########################################################################

The Internet has quickly become a critical infrastructure that our modern society heavily relies on. No longer is the Internet only a carrier of less time- and availability-critical services like e-mail and file transfer, but it now also carries services like e.g.\ e-commerce and healthcare, where even a temporary disruption of service could result in huge economical losses, reduced quality of healthcare or even death in the extreme cases. Failures, however, are unavoidable, and our current Internet needs to be improved to ensure the availability of critical services even when the underlying network fails. One way of doing this is to increase resilience through the use of \emph{multi-homing}. A multi-homed node is connected to more than one \emph{Internet Service Provider}~(ISP), in order to improve availability through the use of multiple Internet connections. As long as at least one ISP is operational, the node and its services should remain available.

The size and complexity of the Internet, together with the increased use of the network as a carrier for critical services, is a barrier for doing changes to the current infrastructure of the network. New ideas, like new protocols and algorithms related to multi-homing, need to be thoroughly tested and verified before they can be deployed. Furthermore, the testing and verification needs to be conducted in a realistic environment for the results to be valid. To satisfy this need in the context of multi-homing, we are realising the distributed multi-homed testbed \noun{NorNet}. Particularly, it will be the first testbed platform that provides the possibility to perform realistic and large-scale multi-homing experiments in the Internet.

The goal of this chapter -- based on~\cite{PAMS2013-NorNet,ComNets2013-Core} -- is to introduce the \noun{NorNet} testbed to the reader. Therefore, the chapter will give a general overview of the testbed, and a detailed description of the design and implementation of the fixed-line part of it, \noun{NorNet Core}. By sharing the implementation details of \noun{NorNet Core} and informing about the capabilities of the testbed, we would like to invite researchers in multi-homing to become future users of the \noun{NorNet} testbed.



% ###########################################################################
\section{The Design}
\label{sec:Design}
% ###########################################################################

The \noun{NorNet} testbed is made up of two parts, \noun{NorNet Edge} and \noun{NorNet Core}. \noun{NorNet Edge} is the wireless part of the \noun{NorNet} infrastructure. It consists of more than 500~multi-homed measurement nodes that are geographically distributed in Norway, each node being connected to five wireless broadband ISPs. \noun{NorNet Core}, on the other hand, which is the main subject of this chapter, consists of a set of multi-homed, programmable sites being interconnected by wired Internet connections.

Figure~\ref{cap:NorNet-Sites-Map} shows the location of the ten first \noun{NorNet Core} sites. Each site consists of four servers and a switch, where one of the servers acts as a so-called \emph{tunnelbox}. While the switch connects all the servers, or \emph{nodes}, locally at a site, the tunnelbox is responsible for connecting its own site to other sites in \noun{NorNet Core}, by using the available set of ISPs (see Figure~\ref{cap:NorNet-Network}). Note that a tunnelbox creates connections, or more precisely IPv4 and IPv6 \emph{tunnels}, by using all possible combinations of local ISPs and remote ISPs at all the other sites' tunnelboxes. That is, the topology of the \noun{NorNet Core} is a fully connected mesh. At each site, \noun{Uninett} (the ISP for the Norwegian research institutions) is used as the default ISP for all administrative purposes.

The additional servers at the sites, that is, the ones not being tunnelboxes, host the virtual resources that are made available for researchers to perform experiments in \noun{NorNet Core}. While there are currently three such servers at each site, it is of course possible to extend their number if there is need for increased research resources. The administration of resources and users is realised by using a private \noun{PlanetLab}~\cite{PBF+05} installation, with a central server located at the Simula Research Laboratory~\cite{SimulaBook} site. Resources are then requested and provided to the researchers as \emph{slices}. A key challenge of the original \noun{PlanetLab} testbed is the low availability of nodes. At the time this section was written, only about~36\% of the \noun{PlanetLab} member nodes were actually available\footnote{We have associated all 1,035~registered nodes to a test slice but were able to login to only~367, i.e.\ about~36\%, of them. Test made on June~28, 2012.} for usage. To avoid a similar situation in \noun{NorNet Core}, a close monitoring and administrative scheme is enforced throughout the testbed. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{center}
\includegraphics[width=0.80\columnwidth]{Figures/PDF/Norway_noLinks_inkl_Svalbard.pdf}
\end{center}
\caption{The \noun{NorNet Core} Sites Map}
\label{cap:NorNet-Sites-Map}
\end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{center}
\includegraphics[width=0.80\columnwidth]{%
   Figures/PDF/NorNetCore-Network.pdf}  
\end{center}
\caption{The \noun{NorNet Core} Network Schema}
\label{cap:NorNet-Network}
\end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% ###########################################################################
\section{The Implementation}
\label{sec:Implementation}
% ###########################################################################

In the following, we describe the implementation of the fixed-line \noun{NorNet Core} testbed.


% ===========================================================================
\subsection{The Testbed Management}
\label{sub:The-Testbed-Management}
% ===========================================================================

\noun{NorNet Core} intends to reuse the \noun{PlanetLab}~\cite{PBF+05} software whenever possible. This software is available in a couple of distributions, most notably the original \noun{PlanetLab} software itself as well as \noun{OneLab}\footnote{\noun{OneLab}: \url{http://www.onelab.eu/}.}. All variants have in common that there is a central administration component, denoted as \emph{PlanetLab Central\nomenclature{PLC}{PlanetLab Central}\index{PlanetLab Central}}~(PLC\index{PLC|see{PlanetLab Central}}, \cite{Hua06}). The PLC manages a database with the configurations of sites, nodes, users and slices. Also, it provides a web-based administration interface as well as an XMLRPC-based API -- called PLCAPI~\cite{PLCAPI} -- for software-based access to the configuration database. A particularly useful feature of the PLC software is to allow for custom extensions by adding so-called \emph{tags} to the database. Tags are additional information to be managed, like e.g.\ testbed-specific configuration parameters.

For \noun{NorNet Core}, it has been straightforward to reuse the PLC as a basis. Custom, \noun{NorNet}-specific configuration is simply realised by appropriate tags. In particular, tags are used to store additional information about the tunnelbox configurations (ISPs, addresses, etc.) and provider networks available at each site. The PLCAPI allows the \noun{NorNet} components -- which will be introduced later -- to access their configuration information and apply it as needed.


% ===========================================================================
\subsection{The Sites}
\label{sub:The-Sites}
% ===========================================================================

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{center}
\subfigure[Schematic View]{
\includegraphics[height=6.00cm]{%
   Figures/PDF/NorNetCore-Site-PhysicalSetup.pdf}  
\label{subcap:Setup-Schema}
}
\subfigure[A Real Deployment]{
\includegraphics[height=6.00cm]{%
   Images/PDF/NorNetCore-Tromsoe.pdf}
\label{subcap:Setup-Real}
}
\end{center}
\caption{The Physical Site Setup}
\label{cap:The-Site-Setup}
\end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Each \noun{NorNet Core} site needs appropriate hardware to realise routing (i.e.\ the tunnelbox functionality) as well as to provide resources for researchers. Therefore, we have decided to deploy the setup depicted in Figure~\ref{cap:The-Site-Setup} at each site; Subfigure~\ref{subcap:Setup-Schema} shows a schematic illustration of a site, while subfigure~\ref{subcap:Setup-Real} shows a picture of a real setup\footnote{Setup at \foreignlanguage{norsk}{Universitetet i Tromsø\index{Universitetet i Tromsø}~(UiT)}, Troms\index{Troms}/Nord-Norge.}.
A site physically consists of one managed switch and four HP~DL320 servers. All servers have an identical configuration (4-core x86\_64-CPU\nomenclature{CPU}{Central Processing Unit}\index{Central Processing Unit}\index{CPU|see{Central Processing Unit}}, 8~GiB RAM, ca.\ 500~GiB harddisk, and two 1000BaseT Ethernet\index{Ethernet} interfaces). Since server~1 (figure~\ref{subcap:Setup-Schema}) is intended to work as a tunnelbox, it also contains an additional network interface card with four 1000BaseT\index{1000BaseT} Ethernet ports. The first of these ports is connected to \noun{Uninett}; the other ports are used for further providers (not yet connected in the depicted setup). The switch connects the first built-in Ethernet port of the servers~1 to~4. This Ethernet hosts the \noun{NorNet}-internal, provider-specific networks (which are realised by Ethernet VLANs\nomenclature{VLAN}{Virtual LAN}\nomenclature{LAN}{Local Area Network}\index{Virtual LAN}\index{VLAN|see{Virtual LAN}}).

The operating system installed on the servers is \noun{Ubuntu Linux}\footnote{\noun{Ubuntu Linux}: \url{http://www.ubuntu.com/}.}\index{Ubuntu}\index{Linux!Ubuntu} 12.04~LTS\nomenclature{LTS}{Long Term Support}\index{Long Term Support}\index{LTS|see{Long Term Support}}, a version with five years support, realised as a quite minimal server installation (to be explained in more detail in Subsection~\ref{sub:The-Software}). The purpose of this installation is to allow for basic configuration tasks, remote login by using \emph{Secure Shell}\nomenclature{SSH}{Secure Shell}\index{Secure Shell}~(SSH\index{SSH|see{Secure Shell}})~\cite{RFC4254} as well as to run \noun{VirtualBox}~\cite{VirtualBoxUserManual} to host virtual machines for all further tasks. Particularly, this means that the tunnelbox, a control system (allowing remote debugging and configuration tasks) and all research systems are realised as virtual machines. This allows for a good utilisation of the available hardware resources, and it is furthermore possible to add and remove systems on demand (e.g.\ for testing, for special kernel-based experiments not being possible with the \noun{PlanetLab} software, etc.). Also, it allows to easily backup and restore the virtual systems, without requiring physical access to the hardware itself. This should make it possible to resolve most problems \emph{quickly}, in order to achieve minimal downtime for the testbed users.

To provide access to a site even when server 1 (hosting the tunnelbox and therefore handling routing to other \noun{NorNet Core} sites and the Internet) is not working properly, the control system on the fourth server is also equipped with a global IP~address in the \noun{Uninett} network (not yet connected in Subfigure~\ref{subcap:Setup-Real}).

For fast deployment, the \noun{Ubuntu Linux} setup for the machines has been preseeded on an installation USB-stick. Preseeding~\cite{UbuntuServerGuide} denotes to automatically apply a pre-defined configuration, i.e.\ partitioning the harddisk, selecting software packages and performing post-installation configuration tasks. That is, to install a server, the only manual tasks are to set up the network and to confirm repartitioning. Once server 1 is able to perform routing, the three other servers can run the installation procedure in parallel. Network access during installation is necessary to ensure up-to-date package versions (including security updates).

While all sites host virtual machines with one tunnelbox, a control system and some research systems (to be explained in detail in Subsection~\ref{sub:The-Research-Systems}), the Central Site -- at the Simula Research Laboratory (see the map in Figure~\ref{cap:NorNet-Sites-Map}) -- also hosts the PLC server (see Subsection~\ref{sub:The-Testbed-Management}) as well as the network monitoring infrastructure (to be introduced in Subsection~\ref{sub:The-Monitoring-Setup}). Also, a file server is available to store virtual machine image backups.


% ===========================================================================
\subsection{The Software}
\label{sub:The-Software}
% ===========================================================================


\noun{Ubuntu Linux}, as a variant of \noun{Debian Linux}, is based on the package manager \emph{Advanced Packaging Tool}~(APT)~\cite{DebianDevelopersReference}. The \noun{Debian} tool-chain allows for easy creation, validation and building of software packages, including automatic handling of software dependencies, where the packages can be provided in a software repository for download. For \noun{Ubuntu}, there is a cloud service provided for this task on \noun{Launchpad}\footnote{\noun{Launchpad}: \url{http://www.launchpad.net/}.}: \emph{Personal Package Archives}~(PPA). Using PPA, it is just necessary to create a source package and upload it into this cloud service. It will then automatically build binary installation packages, which are afterwards provided in a package repository. Since this service has shown to be very convenient, it is also applied for \noun{NorNet Core}. All \noun{NorNet Core} systems just need to refer to the PPA for package installation, in order to obtain the up-to-date \noun{NorNet Core} software.
Furthermore, the APT-based packaging with its support for automatic dependency resolution allows to efficiently break down a larger software system, like \noun{NorNet Core}, into multiple packages. The \noun{NorNet Core} packages are depicted in Figure~\ref{cap:The-Software-Packages}. Please refer to this figure while reading about the packages in the following sections.
% ; they will be introduced in the following subsections.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.80\columnwidth]{%
   Figures/PDF/NorNetCore-Packages.pdf}
\end{center}
\caption{The Software Packages}
\label{cap:The-Software-Packages}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ===========================================================================
\subsection{The Physical Servers}
% ===========================================================================

The installation of a physical server (see Subsection~\ref{sub:The-Sites}) is described by the meta-package ``Server''. It is automatically installed by the preseed procedure and particularly ensures the installation of \noun{VirtualBox}~\cite{VirtualBoxUserManual} for the virtualisation, plus system management tools.
These added system management tools include e.g.\ \noun{Traceroute} (routing debugging), \noun{Whois} (IP ownership query tool), \noun{SubNetCalc}\footnote{\noun{SubNetCalc}: \url{http://www.iem.uni-due.de/~dreibh/subnetcalc/}.} (IP network calculator), \noun{NetPerfMeter}~\cite{SoftCOM2011} (advanced network performance metering tool), \noun{rsplib}~\cite{Dre2006} tools (RSerPool debug tools), \noun{OpenSSH} (SSH server for remote login~\cite{RFC4254}), SNMP tools (for accessing network management information~\cite{RFC3411}) and \noun{GPM}\footnote{\noun{GPM}: \url{http://www.nico.schottelius.org/software/gpm/}.} (mouse support for the console). Since these tools are not only useful for physical servers, they have been grouped in the meta-package ``Management''. It will be installed as a dependency of the ``Server'' package.


% ===========================================================================
\subsection{The Virtual Nodes}
\label{sub:The-Nodes}
% ===========================================================================

All standard virtual \noun{NorNet Core} systems run \noun{Ubuntu Linux} and are based on the ``Node'' package. Similar to ``Server'', it depends on ``Management'' to install the system management tools. Also, ``Node''-based systems need to access the PLCAPI (see Subsection~\ref{sub:The-Testbed-Management}). To make it easier to use the \noun{NorNet}-specific extensions (stored as tags), the ``API'' package contains some \noun{Python} classes and scripts for that purpose.

In addition, the ``Node'' package installs some management scripts. Particularly, \noun{cron-apt}\footnote{\noun{cron-apt}: \url{http://inguza.com/software/cron-apt}.} will be set up to automatically apply APT-based package updates, in order to keep the installation up-to-date. Also, a \noun{cron}-job is installed to regularly query the PLC database for configuration changes. Once there is a configuration update, it will be applied. The following configuration tasks are handled in this way:
\begin{itemize}
 \item The hostname of the system is set.
 \item The IP configuration of the \noun{NorNet}-internal networks is applied.
 \item \emph{Domain Name System}\nomenclature{DNS}{Domain Name System}\index{Domain Name System}~(DNS\index{DNS|see{Domain Name System}})~\cite{RFC1035} server addresses are set.
 \item The \emph{Network Time Protocol}~(NTP) service for time synchronisation is configured (to be explained in Subsection~\ref{sub:The-Tunnelboxes}).
 \item The \emph{Simple Network Management Protocol}~(SNMP)~\cite{RFC3411} service is set up. It allows to query status information of the system within the \noun{NorNet}-internal network. Particularly, it will be used for monitoring (to be explained in Subsection~\ref{sub:The-Monitoring-Setup}).
\end{itemize}

Also, the ``Node'' package installation updates the configuration of the \noun{Grub2} boot manager. Particularly, it sets a higher screen resolution. The to-be-booted Linux system then uses this resolution for its consoles (1024$\times$768 instead of 640$\times$480; this is the maximum possible for non-X11 consoles in \noun{VirtualBox}). This setting is much more suitable for current displays, making local configuration more convenient.


% ===========================================================================
\subsection{The Virtual Tunnelboxes}
\label{sub:The-Tunnelboxes}
% ===========================================================================

Clearly, the purpose of the ``Tunnelbox'' package is to realise the tunnelbox functionality. It depends on the ``Node'' package and just adds the tunnelbox features to the configuration tasks:
\begin{itemize}
 \item The tunnels are set up.
 \item The NTP service is set up.
 \item The DNS service is configured.
\end{itemize}

% ---------------------------------------------------------------------------
\subsubsection{Tunnel Setup}
% ---------------------------------------------------------------------------

The first part of the tunnel setup obviously consists of configuring the tunnels themselves. For IPv4 tunnels, \emph{Generic Route Encapsulation}~(GRE)~\cite{RFC2784} is applied. This is a well-known standard for tunnelling over IPv4; it is therefore assumed that, in case of future extensions, it can also be handled by firewalls/middleboxes at the gateway to an ISP. IPv6 on the other hand, directly uses tunnelling over IPv6. If there is no IPv6 interconnection provided by the ISPs of two tunnelboxes, IPv6 will just be multiplexed, together with IPv4, over the GRE tunnel.

After setting up the tunnels, appropriate routing has to be configured. By default, routing is based on the longest prefix match for the destination address as well as on the routing metric. For multi-homed systems, however, this is not sufficient. That is, all packets for the same destination IP~address would take the same route (via the same ISP, of course). To support multi-homing, routing also needs to consider the source address. For example, if a packet has a source IP address of the ISP \noun{Uninett}, the packet should go over the interface to \noun{Uninett}. On the other hand, a packet having a source IP from the ISP \noun{Telenor} should be sent out via \noun{Telenor}. This functionality is realised by applying IP-rules~\cite{IPLayerNetworkAdministrationWithLinux}: an own routing table is configured for each ISP. Then, IP rules select the routing table for a packet based on its source address.

In order to give the researcher even more control over the outgoing interface, IP~rules realise another feature: by setting the \emph{Type of Service}~(TOS) field of IPv4 packets/Traffic Class of IPv6 packets to certain values, the sender can ``override'' the outgoing ISP. That is, even if a packet has a source address in the \noun{Telenor} network, the researcher can decide to send it via the \noun{Uninett} interface. Due to a limitation in the TOS/Traffic Class handling within the Linux kernel, it is only possible to use seven combinations of values. For the ISP selection based on TOS/Traffic Class, this limits the number of outgoing ISPs per site to seven. This is assumed to be sufficient, and -- with a patched kernel -- it could also be increased to~63, if really needed.
An alternative approach for realising the routing could be to apply \noun{OpenFlow}~\cite{OpenFlowSpecification1.2}, in form of the \noun{Open vSwitch}\index{Open vSwitch}~\cite{PGP+10} extension for Linux.

% ---------------------------------------------------------------------------
\subsubsection{Time Synchronisation}
% ---------------------------------------------------------------------------

For many network measurement tasks, it is quite handy to e.g.\ measure the one-way delay. For that purpose, however, exact time synchronisation between the sender side and the receiver side of a packet flow is necessary. NTP~\cite{RFC5905} is the IETF standard for time synchronisation. Since a tunnelbox is available locally at each site, and it is running on a dedicated machine, the NTP daemon of a tunnelbox is chosen to act as a local server for the site. That is, the systems within the local network can use it for synchronisation. Furthermore, the NTP daemons of all sites act as peers. That is, they form a \noun{NorNet}-internal network for time synchronisation. In addition, the NTP daemon at the Central Site uses the external NTP servers of the Physikalisch-Technische Bundesanstalt\footnote{Physikalisch-Technische Bundesanstalt: \url{http://www.ptb.de/}.}~(PTB) in Braunschweig/Germany for time synchronisation. Based on atomic clocks, the PTB provides the official time reference for Central Europe. It can therefore be considered to be a highly reliable and trustworthy time source.

% ---------------------------------------------------------------------------
\subsubsection{Name Resolution}
\label{subsub:Tunnelbox-Name-Resolution}
% ---------------------------------------------------------------------------

A tunnelbox also acts as a DNS server~\cite{RFC1035} -- by using the \noun{BIND}~\cite{Bind9ARM} package -- for its local site, providing caching for external DNS resolutions. Furthermore, in order to help researchers working with the \noun{NorNet Core} private network addresses, all interfaces also have a DNS name below the private \texttt{nornet.}\ top-level domain~(TLD). Each site has its own second-level domain below this TLD, e.g.\ \texttt{simula.nornet.}\ is the domain of the Central Site as the Simula Research Laboratory. Each site's tunnelbox is the master DNS of its second-level domain; the Central Site's DNS acts as slave. For the Central Site (managing the TLD and the \texttt{simula.nornet.} second-level domain), the other sites' tunnelboxes act as slaves. This should ensure a sufficient resilience and load distribution.

A further feature of the \noun{NorNet Core} DNS service is that it provides LOC resource records~\cite{RFC1876} for all  \noun{NorNet}-internal DNS names. These records provide the geographic location of the names, allowing researchers to easily access this information and make use of it.


% ===========================================================================
\subsection{The Research Systems}
\label{sub:The-Research-Systems}
% ===========================================================================

The research systems are the virtual machines that researchers use for their experiments. Clearly, they are the purpose \noun{NorNet Core} has been built for. In many cases, it is -- with extensions to make use of multi-homing -- just sufficient to use \noun{PlanetLab}-based virtual systems here. Systems based on this software~\cite{PBF+05} obtain their configurations from the PLC and install automatically. It is just sufficient to obtain a bootstrap CD-ROM image (from the PLC) and boot a virtual \noun{PlanetLab}-based research system with this image.

A system based on the \noun{PlanetLab}\index{PlanetLab} software runs slivers (i.e.\ virtual machines) for the experiments. Originally, the \noun{PlanetLab} software -- as well as the different variants of it -- have applied \noun{Linux VServer}\footnote{\noun{Linux VServer}: \url{http://linux-vserver.org/}.} for this purpose. However, \noun{Linux VServer} is not in the mainline kernel, which makes kernel updates difficult. Particularly, this has been the reason why the stable \noun{PlanetLab} distributions are based on quite out-of-date variants of \noun{Fedora Core Linux}\index{Fedora Core}\index{Linux!Fedora Core} (e.g.\ version~14 for \noun{OneLab}\index{OneLab}; version~12 for original \noun{PlanetLab}). The currently ongoing development direction of the \noun{PlanetLab} software is therefore to replace \noun{Linux VServer} with \noun{Linux Containers}\footnote{\noun{Linux Containers}: \url{http://lxc.sourceforge.net/}.}~(LXC). Particularly, LXC is fully included in the mainline Linux kernel, making it easy to deploy state-of-the-art kernels as well as underlying Linux distributions.

At the moment, there are already experimental builds of the LXC-based \noun{PlanetLab} software available\footnote{LXC-based \noun{PlanetLab} build: \url{http://build.onelab.eu/lxc/}.}. Besides the fact that it will support to assign an own IPv4 address per sliver (which avoids complicated separation of flows among slivers), it will also support IPv6. Due to these advantages, in combination with the very active development, we are intending to base \noun{NorNet Core} on the LXC-based software. Also, we are actively involved within the development community, by contributing tests of and improvements to the new software.

Slices -- regardless of whether they are based on LXC or \noun{Linux VServer} -- provide lightweight virtual Linux systems for the researchers. Particularly, there is only a single Linux kernel running. All slices running on the system share it; changes to the kernel would therefore affect \emph{all} slices of that system. However, in some cases, it might be useful to run full virtual systems, e.g.\ in order to deploy custom kernels to perform evaluations of CMT-SCTP under FreeBSD~\cite{Dre2012,PAMS2012} or kernel-based Linux MPTCP~\cite{RBP+11}. In this case, further \noun{VirtualBox}-based full virtual machines could be configured on the physical servers. In the future, the instantiation of virtual machines to boot custom kernels could e.g.\ be automatised by a system like \noun{ToMaTo}~\cite{SHG+11}.


% ===========================================================================
\subsection{The Monitoring Setup}
\label{sub:The-Monitoring-Setup}
% ===========================================================================

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.98\columnwidth]{%
   Images/PDF/Kontrollsenteret.pdf}
\end{center}
\caption{``NorNet-Kontrollsenter'' -- A Screenshot of the Control Center Display}
\label{cap:Kontrollsenteret}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to achieve the testbed availability described in Section~\ref{sec:Design}, tight monitoring is necessary to detect problems early and to trigger their resolution as soon as possible. The basis of our monitoring infrastructure is a \noun{Nagios}~\cite{NagiosCoreDocumentation} setup, which is deployed in a virtual machine at the Central Site. The ``Monitor'' package provides this functionality, based on the ``Node'' package (see Subsection~\ref{sub:The-Nodes}). \noun{Nagios} actively obtains the status of all components, e.g.\ the systems themselves with \noun{Ping} tests, and services like the PLC's web server by HTTP checks. Particularly, \noun{Nagios} allows a fine-granular configuration of warning and error conditions, as well as the extension to custom services (e.g.\ to check certain system-dependent status values by using SNMP) in form of plug-ins. Such plug-ins have been written to perform checks of sites and tunnels. The monitoring results are recorded by \noun{Nagios}; a web interface provides access for administrators.

While the web interface presents a very detailed and fine-grained view of \noun{NorNet Core}, a more illustrative view was intended to give a quick overview of the site statuses for both administrators and users of the testbed (i.e.\ the researchers). Therefore, we -- as a first step -- applied the \noun{NagMap}\footnote{\noun{NagMap}: \url{https://github.com/hecko/nagmap/}.} plug-in for \noun{Nagios}, which provided a map view of the \noun{NorNet Core} testbed. However, \noun{NagMap} just produced a quite simple projection of the sites and their connections (i.e.\ the tunnels) to a \noun{Google Maps}\footnote{\noun{Google Maps}: \url{https://maps.google.no/}.} satellite map. Particularly due to the fully-connected mesh topology of the tunnels (see Section~\ref{sec:Design}), the link view, however, became relatively confusing. Therefore, we have developed our own plug-in named \noun{NorNet-Kontrollsenter}.

\noun{NorNet-Kontrollsenter} uses \noun{Apache}/PHP~\cite{ApacheDoc} to access the recorded \noun{Nagios} status information. It then provides an HTML page with the results. That is, the plug-in runs on the monitoring system itself. The results can then be accessed from a web server running on the same system, while a web browser on another machine can display the corresponding webpage (as shown in Figure~\ref{cap:Kontrollsenteret}). The returned web page with the results makes use of JavaScript, AJAX (Asynchronous JavaScript and XML) as well as SVG (Scalable Vector Graphics) with JavaScript to provide dynamic updates without having to reload the whole page. Furthermore, instead of tying the map display to a certain map service, we apply \noun{OpenLayers}~\cite{OpenLayersDoc}. It provides a service-independent mapping and layering library in JavaScript; the display can dynamically change between map services (currently, we apply 
\noun{Open Street Map}\footnote{\noun{Open Street Map}: \url{http://www.openstreetmap.org/}.},
\noun{Google Maps} and
\noun{Bing Maps}\footnote{\noun{Bing Maps}: \url{http://www.bing.com/maps/}.}). Furthermore, overlaying functionality is provided. That is, overlays like the \noun{Open Weather Map}\footnote{\noun{Open Weather Map}: \url{http://www.openweathermap.org/}.} overlay can add additional information to the map (in Figure~\ref{cap:Kontrollsenteret}: the current weather as well as clouds and precipitation forecasts). In the future, we plan to use this feature for more detailed network status information. Map services and the visibility of overlays can be changed interactively.

For improved visibility, we only display a connection to the Central Site (in green colour) if all tunnels of a site are working. Only non-working tunnels would result in yellow (warning level) or red (error level) coloured links between sites. Connections to not-yet-deployed sites are represented by grey, dashed lines.

Since the access to the monitoring results is browser-based, it is possible to run multiple browser instances simultaneously. We use this feature e.g.\ to display a map of the Norwegian mainland (see Figure~\ref{cap:Kontrollsenteret}) on a large screen as well as a map of Svalbard (more than 1,000~km north of the mainland) on a separate, small screen. The ``Display'' package extends the ``Node'' setup with configurations for such a display machine, by adding an X~server and the \noun{Firefox} web browser.


% -> mobility support in the future
% -> GUI Localisation



% ###########################################################################
\section{Testbed Deployment and Research Ideas}
% ###########################################################################

Currently, we are in the deployment phase of the sites~\cite{Karlstad2012}. That is, the hardware setup described in Subsection~\ref{sub:The-Sites} has been installed at 7~sites, with 3~further sites in the following weeks.

For the research part of \noun{NorNet Core}, there are obviously use cases for research on resilience (e.g.\ in the context of RSerPool~\cite{IJIIDS2010,Dre2006,rfc-rserpool-overview}, etc.). Furthermore, there is a strong interest in using the testbed in the context of multi-path transport performance with MPTCP~\cite{RBP+11} and CMT-SCTP~\cite{PAMS2012}, as well as for congestion control evaluations~\cite{Globecom2013,ICC2012,YWY08a} in that context. Particularly, there is a growing interest in CMT-SCTP, since \emph{Real-Time Collaboration on the World Wide Web}~(RTCWEB)~\cite{draft-ietf-rtcweb-overview} is based on SCTP. RTCWEB is a framework for browser-to-browser multimedia communications (e.g.\ video conferencing) that is currently under standardisation in the IETF and therefore a very hot research topic. With SCTP being deployed by possibly billions of web browsers on frequently multi-homed endpoints (e.g.\ tablet PCs and smartphones with 3G and WLAN interfaces), the usage of multi-path transport might become very tempting. 



% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
\chapter{Getting Access}
\label{cha:Getting-Access}
% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

In this chapter, the user part of accessing and using \noun{NorNet Core} is described. The maintenance part will be described in Chapter~\ref{cha:Maintenance}.


% ###########################################################################
\section{User Accounts}
\label{sec:User-Accounts}
% ###########################################################################

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\begin{center}
\begin{tabular}{|l||l|l|}
 \hline
 Server             & User Name  & Password \\ \hline
 \hline
 \href{ssh://gatekeeper.nntb.no}{gatekeeper.nntb.no}\index{gatekeeper.nntb.no}~(Gatekeeper) & ola                                                               & some-secret-password \\ \hline
 \href{https://plc.simula.nornet}{plc.simula.nornet}\index{plc.simula.nornet}~(PLC)         & \href{mailto:ola.nordmann@example.com}{ola.nordmann@example.com}  & another-password     \\ \hline
\end{tabular}
\end{center}
\caption{An Example Accounts Table}
\label{tbl:An-Example-Accounts-Table}
\end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A new \noun{NorNet Core} user will at least receive the following accounts:
\begin{itemize}
 \item The Gatekeeper account providing access to a Linux machine that is connected to \noun{NorNet Core} and the outside Internet.
 \item The PLC account providing access to the PLC (see Subsection~\ref{sub:The-Testbed-Management}). Note, that the PLC user name \emph{must} be a valid e-mail address of the user.
\end{itemize}
Possibly, further accounts will be provided, e.g.\ for web server, wiki server, etc.. Such accounts are optional and only provided when needed.
Table~\ref{tbl:An-Example-Accounts-Table} shows an example for the user Ola Nordmann.
Note, that users are \emph{required} to change their initial passwords as soon as possible!



% ###########################################################################
\section{Getting Access into the \noun{NorNet Core} Network}
\label{sec:Access-into-NorNet-Core-Network}
% ###########################################################################

A \noun{NorNet Core} user needs access into the \noun{NorNet Core} network. The possibilities for such accesses are introduced in the following subsections.


% ===========================================================================
\subsection{The Gatekeeper}
\label{sub:The-Gatekeeper}
% ===========================================================================

The Gatekeeper server\index{Gatekeeper} \href{ssh://gatekeeper.nntb.no}{gatekeeper.nntb.no}\index{gatekeeper.nntb.no}\footnote{\href{ssh://gatekeeper.nntb.no}{gatekeeper.nntb.no}\index{gatekeeper.nntb.no} is actually an alias for the server \href{ssh://oesthorn.nntb.no}{oesthorn.nntb.no}\index{oesthorn.nntb.no}, to be explained in Section~\ref{sec:The-Gatekeeper}.} provides the simplest form of access to the \noun{NorNet Core} network. It is just a Linux machine that is connected to the \noun{NorNet Core} network as well as to the outside Internet. SSH\index{Secure Shell} can be used to access this machine. In the following, some useful commands that are provided by standard Linux distributions are explained. Note, that access to the Gatekeeper is not restricted to Linux. In fact, any operating system with appropriate tools (SSH, etc.) can be used as well.

The following examples give a short overview of what is possible with SSH. Note, that they are not meant to be complete. Refer to the manual page of \texttt{ssh}\footnote{\texttt{ssh} Manual Page: \texttt{\href{man:ssh}{man ssh}}.} and various SSH tutorials in the Internet for further information.


% ---------------------------------------------------------------------------
\subsubsection{Using Secure Shell}
\label{subsub:Gatekeeper-Using-SSH}
% ---------------------------------------------------------------------------

An SSH remote shell connection to the Gatekeeper server\index{Gatekeeper!Secure Shell} can be established e.g.\ by the \texttt{ssh}\index{ssh} command:
\begin{lstlisting}
ssh ola@gatekeeper.nntb.no
\end{lstlisting}
for user ``ola''. Of course, the user has to set his own Gatekeeper username here. For login, the corresponding  is required. Key-based authentication will be explained in Subsubsection~\ref{subsub:Key-Based-Authentication}.


% ---------------------------------------------------------------------------
\subsubsection{Changing the Password}
% ---------------------------------------------------------------------------

Once logged in the the Gatekeeper server, the user can change the password\index{Password}\index{Gatekeeper!Password Change} with the \texttt{passwd} command. It asks for the old password and twice for the new one. If both inputs of the new password are equal, the password is updated. The user is required to chose a ``good'' password for the Gatekeeper, since it is exposed to the outside Internet.


% ---------------------------------------------------------------------------
\subsubsection{Using Secure Copy}
% ---------------------------------------------------------------------------

Secure Copy\nomenclature{SCP}{Secure Copy}\index{Secure Copy}~(SCP\index{SCP|see{Secure Copy}}) uses SSH to transfer files between machines, i.e.\ it is possible to copy files from the local machine to the Gatekeeper\index{Gatekeeper!Secure Copy} and vice versa. The corresponding command is named \texttt{scp}\index{scp}.

To copy a file \texttt{test.data} from the local machine to the Gatekeeper server, use for example:
\begin{lstlisting}
scp test.data ola@gatekeeper.nntb.no:
\end{lstlisting}
Note the colon (i.e.\ ``:'')! Without this colon, ssh would behave like a local file copy (i.e.\ \texttt{cp}\index{cp}) and just create a local copy of \texttt{test.data} with name \text{ola@gatekeeper.nntb.no}. \texttt{scp} will store the file in the home directory (the default place) of the Gatekeeper server. A different directory can be specified after the colon, e.g.\ \texttt{directory/subdirectory} (note, that these directories must exist):
\begin{lstlisting}
scp test.data ola@gatekeeper.nntb.no:directory/subdirectory/
\end{lstlisting}
It is furthermore possible to provide a file name for the destination (e.g.\ \texttt{my-first-test.data}):
\begin{lstlisting}
scp test.data ola@gatekeeper.nntb.no:directory/my-first_test.data
\end{lstlisting}
\texttt{scp} can also copy complete directories, by using the ``-r'' option for a recursive copy:
\begin{lstlisting}
scp -r test-directory ola@gatekeeper.nntb.no:
\end{lstlisting}
This will copy the whole directory \texttt{test-directory}, including all of its subdirectories, to the Gatekeeper.

Similarly, the reverse direction -- i.e.\ from the Gatekeeper to the local machine -- can be handled as well:
\begin{lstlisting}
scp ola@gatekeeper.nntb.no:test.data .
scp ola@gatekeeper.nntb.no:test.data some-local-directory/
scp -r ola@gatekeeper.nntb.no:my-test-directory/ .
\end{lstlisting}
Of course, copying from a remote to another remote machine would work as well.
For further possibilities, also see the manual page of \texttt{scp}\footnote{\texttt{scp} Manual Page: \texttt{\href{man:scp}{man scp}}.}.


% ---------------------------------------------------------------------------
\subsubsection{Using Remote File Synchronisation}
% ---------------------------------------------------------------------------

SCP only copies files and directories. When working with \noun{NorNet Core} experiment results, it becomes handy to synchronise files and directories, i.e.\ to just transfer data that needs to be transferred. This functionality is provided by the texttt{rsync}\index{rsync} command\index{Gatekeeper!Remote Synchronisation}. See also the manual page of \texttt{rsync}\footnote{\texttt{rsync} Manual Page: \texttt{\href{man:rsync}{man rsync}}.} for details.

In order to synchronise the directory \texttt{experiment1/results/} from the Gatekeeper to the local machine, use for example:
\begin{lstlisting}
rsync -a -v -z --partial --progress --delete \
 ola@gatekeeper.nntb.no:experiment1/results/ experiment1/results/
\end{lstlisting}
The following options are of interest:
\begin{itemize}
 \item ``-a'': recursive synchronisation.
 \item ``-v'': verbose output.
 \item ``-z'': use compression for improving the transfer speed over slow connections. However, it comes of course at the cost of increased CPU utilisation.
 \item ``-\--partial'': allow to resume a file transfer after interruption. For example, having transferred 99\% of 1~GiB just needs the transfer the remaining 1\% after aborting and restarting \texttt{rsync}. Otherwise, the transfer of this file would restart from scratch.
 \item ``-\--progress': shows the progress of the transfer.
 \item ``-\--delete'': deletes files that are not existing any more in the source directory. Warning: this option may be disastrous when mistakenly providing the wrong destination (e.g.\ the user's home directory). Use this option with care!
\end{itemize}

Similarly, data can be synchronised from the local machine to the Gatekeeper:
\begin{lstlisting}
rsync -a -v -z --partial --progress experiment1/ \
 ola@gatekeeper.nntb.no:experiment1/
\end{lstlisting}


% ---------------------------------------------------------------------------
\subsubsection{Configuring Key-Based Authentication}
\label{subsub:Key-Based-Authentication}
% ---------------------------------------------------------------------------

Instead of typing the password for login each time, the user can also create a SSH public/private key pair on the local machine (i.e.\ the machine used to access the Gatekeeper server). By storing the public key on the Gatekeeper server in \texttt{.ssh/authorized\_keys}, the user can use his private key to authenticate to the Gatekeeper instead of using his password. This is particularly useful when using SSH in scripts.

To create a public/private key pair, the command \texttt{ssh-keygen}\index{ssh-keygen} is used:
\begin{lstlisting}
ssh-keygen -P "" -f ~/.ssh/gatekeeper
\end{lstlisting}
The following options are of interest:
\begin{itemize}
 \item ``-P'': specifies the passphrase for the private key. "" sets no passphrase; this is necessary to use the key in scripts later.
 \item ``-f'': provides the output file prefix. In this case, the private key will be stored in \texttt{\textasciitilde/.ssh/gatekeeper} and the corresponding public key in \texttt{\textasciitilde/.ssh/gatekeeper.pub}.
\end{itemize}
Also see the manual page\footnote{\texttt{ssh-keygen} Manual Page \href{man:ssh-keygen}{\texttt{man ssh-keygen}}.} of \texttt{ssh-keygen} for further options.


The public key (in this example: \texttt{\textasciitilde/.ssh/gatekeeper.pub}) is just a text file with one long line. It can be displayed e.g.\ with:
\begin{lstlisting}
cat ~/.ssh/gatekeeper.pub
\end{lstlisting}
It should be something like ``ssh-rsa <characters and numbers> <user@host>''.

To use this public key for authentication, log into the Gatekeeper and append the key line to the file \texttt{\textasciitilde/.ssh/authorized\_keys}. This can e.g.\ by done by an arbitrary text editor that is available on the Gatekeeper, e.g.\
\texttt{emacs}\index{emacs},
\texttt{joe}\index{joe},
\texttt{nano}\index{nano} or
\texttt{vi}\index{vi}. If the file \texttt{\textasciitilde/.ssh/authorized\_keys} does not yet exist, it can just be created. After creating the file, it is necessary to check the access permissions with \texttt{chmod}\index{chmod}:
\begin{lstlisting}
chmod 600 ~/.ssh/authorized_keys
\end{lstlisting}
This ensures that only the owner (and root, of course) can read or modify it. See also the manual page\footnote{\texttt{chmod} Manual Page \href{man:chmod}{\texttt{man chmod}}.} of \texttt{chmood}. Note, that without secure access permissions, the SSH daemon will refuse to apply the key configuration!

The Gatekeeper should now be ready for public/private key-based authentication. From the local machine, now use
\begin{lstlisting}
ssh -i ~/.ssh/gatekeeper ola@gatekeeper.nntb.no:
\end{lstlisting}
to log into the Gatekeeper. ``-i'' specifies the private key that SSH should use for authentication. If everything is working, \texttt{ssh} should log into the Gatekeeper without need for entering the password. Note, that SSH may not complain if the private key is not found. Instead, it will try password-based authentication. If this is the case, check the provided file name of the private key.


% ---------------------------------------------------------------------------
\subsubsection{Tunnelling TCP Connections}
\label{subsub:Tunnelling-TCP-Connections}
% ---------------------------------------------------------------------------

SSH provides the feature of tunnelling TCP connections via an SSH session. That is, for a given TCP port on the local machine, SSH tunnels the traffic to the remote SSH endpoint (here: the Gatekeeper). From there, it is forwarded to the given destination.  For \noun{NorNet Core}, this is particularly useful to get access to the PLC (to be explained in Section~\ref{sec:The-PLC}) and the Monitor server (to be explained in Section~\ref{sec:Kontrollsenteret}):
\begin{lstlisting}
ssh ola@gatekeeper.nntb.no \
 -L 2000:plc.simula.nornet:443 \
 -L 2001:monitor.simula.nornet:80
\end{lstlisting}
This code example does the following:
\begin{itemize}
 \item An SSH connection is established to the Gatekeeper server \href{ssh://gatekeeper.nntb.no}{gatekeeper.nntb.no}\index{gatekeeper.nntb.no}, as explained in Subsubsection~\ref{subsub:Gatekeeper-Using-SSH}.
 
 \item TCP port~2000 of the local machine (i.e.\ the machine running this command) is forwarded via the Gatekeeper to port~443 of the PLC server \href{https://plc.simula.nornet}{plc.simula.nornet}\index{plc.simula.nornet}. Note, that \href{https://plc.simula.nornet}{plc.simula.nornet}\index{plc.simula.nornet} is the DNS name of the PLC. Since the Gatekeeper uses the \noun{NorNet Core} DNS service, it can resolve this \noun{NorNet}-internal name (see Subsubsection~\ref{subsub:Tunnelbox-Name-Resolution}).
 
 \item Similarly, TCP port~2001 of the local machine is forwarded to port~80 of the Monitor server \href{http://monitor.simula.nornet}{monitor.simula.nornet}\index{monitor.simula.nornet}.
\end{itemize}
By establishing a TCP connection to port~2000 or port~2001 of the local machine, the user can now access the \noun{NorNet}-internal services of the PLC or the Monitor server (to be explained later in Section~\ref{sec:The-PLC} and Section~\ref{sec:Kontrollsenteret}).

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}
\lstinputlisting{CodeSnippets/Gatekeeper.sh}
\caption{A Gatekeeper Access Script Example}
\label{lst:Gatekeeper-Access-Script-Example}
\end{algorithm}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since the establishment of the port forwarding above is a recurring task for almost all \noun{NorNet Core} users, it is strongly recommended to write a small script to perform this task. An example is provided in Listing~\ref{lst:Gatekeeper-Access-Script-Example}.



% ===========================================================================
\subsection{Getting Physical Access}
\label{sub:Getting-Physical-Access}
% ===========================================================================

A user can get physical access to \noun{NorNet Core} by connecting devices to his local site's \noun{NorNet Core} switch (see Figure~\ref{cap:The-Site-Setup}). This ensures easy access and provides the lowest latency and greatest flexibility.
The local network provides automatic IPv4 configuration of the primary ISP by the Dynamic Host Configuration Protocol\nomenclature{DHCP}{Dynamic Host Configuration Protocol}\index{Dynamic Host Configuration Protocol}~(DHCP\index{DHCP|see{Dynamic Host Configuration Protocol}}, \cite{RFC2131,RFC2132}). Note, that DHCP does \emph{not} provide addresses for further ISPs. DHCP will also provide the DNS address (i.e.\ the tunnelbox, see Subsubsection~\ref{subsub:Tunnelbox-Name-Resolution}) so that \noun{NorNet}-internal names can be resolved. Note furthermore, that TCP access with port~80 to non-local endpoints is proxied via the local tunnelbox's Squid proxy (transparent proxy), in order to provide caching.

For IPv6, addresses for all ISPs are be provided by stateless auto-configuration via Internet Control Message Protocol version~6\nomenclature{ICMPv6}{Internet Control Message Protocol version~6}\index{Internet Control Message Protocol version~6}~(ICMPv6\index{ICMPv6|see{Internet Control Message Protocol version~6}}, \cite{RFC4443}).

Note, that dynamic configuration is intended for short-term usage. For long-term usage of devices within the \noun{NorNet Core} network, it is required to configure permanent address mappings for DHCP as well as DNS names. See ******* for the maintenance details!



% ===========================================================================
\subsection{The Virtual Private Network Access}
\label{sub:The-VPN-Access}
% ===========================================================================

An access to the \noun{NorNet Core} network via Virtual Private Network\nomenclature{VPN}{Virtual Private Network}\index{Virtual Private Network}~(VPN\index{VPN|see{Virtual Private Network}}) is planned but not yet implemented.



% ###########################################################################
\section{The PLC}
\label{sec:The-PLC}
% ###########################################################################

The PLC\index{PlanetLab Central} user interface used for \noun{NorNet Core} is mostly based on the upstream \noun{PlanetLab}~\cite{PR06} software. It is therefore only very briefly introduced here.


% ===========================================================================
\subsection{Accessing the PLC Web Interface}
\label{sub:Accessing-the-PLC-Web-Interface}
% ===========================================================================

First, it is necessary to get access to the PLC web interface via a web browser. There are two possibilities:
\begin{itemize}
 \item \url{https://plc.simula.nornet}:
 In case of connection to the local \noun{NorNet Core} network including its DNS service, the web browser can directly connect to the PLC.
 
 \item \url{https://localhost:2000}:
 If access to the \noun{NorNet Core} network is only available via the Gatekeeper server, the SSH port forwarding -- as described in Subsubsection~\ref{subsub:Tunnelling-TCP-Connections} -- can be used. Then, if using the settings described there, the web browser connects to the local machine's port~2000. This port is forwarded, via SSH and the Gatekeeper server, to the PLC.
\end{itemize}

The PLC uses a self-signed Transport Layer Security\nomenclature{TLS}{Transport Layer Security}\index{Transport Layer Security}~(TLS\index{TLS|see{Transport Layer Security}}) certificate. When establishing a connection for the first time, this certificate has to be accepted for the web browser.


Currently, the management of sites and nodes (particularly e.g.\ adding sites or nodes) is made directly by the \noun{NorNet Core} staff at the Simula Research Laboratory.
The PLC web interface can be used by \noun{NorNet Core} users to perform the following tasks:
\begin{itemize}
 \item Managing the PLC account,
 \item Viewing sites and nodes information,
 \item Managing slices.
\end{itemize}
These tasks will be explained in the following subsections.


% ===========================================================================
\subsection{Managing the User Account}
\label{sub:Managing-the-Account}
% ===========================================================================

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.90\columnwidth]{Images/PDF/Screenshot-PLC-Account-Details1.pdf}
\end{center}
\caption{The PLC: My Account $\rightarrow$ Details}
\label{cap:PLC-Account-Details-User}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The own account is managed under ``My Account''. Figure~\ref{cap:PLC-Account-Details-User} shows an example screenshot. Particularly, the user can revise title, first and last name and biography information. Also, the user can change his own password. Since the PLC is used my many users and misuse of an account may affect the whole testbed, all user are \emph{required} to choose ``good'' passwords!

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.90\columnwidth]{Images/PDF/Screenshot-PLC-Account-Details2.pdf}
\end{center}
\caption{My Account $\rightarrow$ Slices, Key and Roles}
\label{cap:PLC-Account-Details-Key-and-Roles}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Further information under ``My Account'' provides mappings of the user to one or more \noun{NorNet Core} sites, mappings to slices and user roles. Since these functionalities are equal to \noun{PlanetLab}~\cite{PR06}, they are not introduced in detail here. Crucial for the access to \noun{NorNet Core} slivers is of course to provide an own SSH public key to the PLC. Like for using the Gatekeeper server with private/public key-based authentication (as introduced in Subsubsection~\ref{subsub:Key-Based-Authentication}), key-based authentication is used for accessing slivers. That is, the PLC provides the users' public keys to research nodes. The research nodes configure \texttt{authorized\_keys} with these keys for the slivers of the users. Then, the users can use their private key to authenticate to their slivers.

The own SSH public can be uploaded in the ``Key'' section of the ``My Account'' view, as shown in Figure~\ref{cap:PLC-Account-Details-Key-and-Roles}. Note, that multiple keys can be provided here. However, the PLC user interface currently only supports a single key for authentication. Therefore, only the first key will be used. The PLC does not print a warning when having uploaded multiple keys! It is therefore strongly recommended to not have more than one key in the interface. A further pitfall for new users is trying to upload the private key here. The PLC will in this case complain about the key length, but not about using a private key.

After upload of the public key, it may take up to about 1~hour until the new key gets distributed to all \noun{NorNet Core} research nodes. Key updates are made by the nodes in random intervals, i.e.\ changes made in the PLC web interface are not applied immediately!


% ===========================================================================
\subsection{Sites}
\label{sub:Sites}
% ===========================================================================

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.90\columnwidth]{%
   Images/PDF/Screenshot-PLC-Sites.pdf}
\end{center}
\caption{Sites}
\label{cap:PLC-Sites}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Information about the available \noun{NorNet Core} sites\index{Sites} are provided under the ``Sites'' section, as shown in Figure~\ref{cap:PLC-Sites}. The overview table just contains the sites' full names (e.g.\ Simula Research Laboratory\index{Simula Research Laboratory}) and abbreviation (e.g.\ SRL). Details on a site can be found by clicking on a site's label.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.90\columnwidth]{%
   Images/PDF/Screenshot-PLC-Sites-HU.pdf}
\end{center}
\caption{Sites $\rightarrow$ Hainan University~(HU)}
\label{cap:PLC-Sites-HU}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~\ref{cap:PLC-Sites-HU} presents the detailed site information view for the Hainan University\index{Hainan University} site. Particularly, it contains data such as the site's URL, the location, the research nodes and the users at at site. \noun{NorNet}-specific information is stored in custom tags. Some important tags are:
\begin{itemize}
 \item ``nornet\_is\_managed\_site''\index{nornet\_is\_managed\_site}: Denoted whether the site is a \noun{NorNet Core}-managed site~(1) or not~(0). \noun{NorNet Core}-managed sites are assumed to be modified by scripts on the PLC server (by the \noun{NorNet Core} staff). Therefore, configuration of the sites should \emph{not} be changed in the PLC web interface (even if the user's role allows to do this) to avoid misconfiguration!

 \item ``nornet\_site\_utf8''\index{nornet\_site\_utf8}: The site name in UTF-8 encoding, e.g.\ \foreignlanguage{norsk}{Universitetet i Tromsø\index{Universitetet i Tromsø}} or \foreignlanguage{german}{Universität Kaiserslautern\index{Universität Kaiserslautern}}. For compatibility reasons with \noun{PlanetLab}, the site name in the site configuration must be ASCII. Therefore, names with special characters (e.g.\ Norwegian, German, etc.) can be stored in this tag.
 
 \item ``nornet\_site\_index''\index{nornet\_site\_index}: The \noun{NorNet Core} site index (see Chapter~\ref{cha:Sites}).
 
 \item ``nornet\_site\_domain''\index{nornet\_site\_domain}: The \noun{NorNet Core}-internal domain name (e.g.\ ``hu.nornet'' for the site at Hainan University\index{Hainan University}).
 
 \item ``nornet\_site\_city''\index{nornet\_site\_city}: The city of the site (e.g.\ Tromsø\index{Tromsø}), in UTF-8.
 
 \item ``nornet\_site\_province''\index{nornet\_site\_province}: The province of the site (e.g.\ Troms\index{Troms}), in UTF-8.
 
 \item ``nornet\_site\_country''\index{nornet\_site\_country}: The full country name of the site (e.g.\ Norge), in UTF-8.
 
 \item ``nornet\_site\_country\_code''\index{nornet\_site\_country\_code}: The country code of the site, e.g.\ NO.
 
 \item ``nornet\_site\_altitude''\index{nornet\_site\_altitude}: The site's altitude, in meters.
 
 \item ``nornet\_site\_contact$N$''\index{nornet\_site\_contact$N$}: Site site's $N$-th technical contact, given as ``Family Name, Given Name: e-mail@domain.tld'', in UTF-8.
 
 \item ``nornet\_site\_default\_provider\_index''\index{nornet\_site\_default\_provider\_index}: The provider index of the primary ISP (see Chapter~\ref{cha:Providers}).
 
 \item ``nornet\_site\_tb\_internal\_interface''\index{nornet\_site\_tb\_internal\_interface}: The tunnelbox's interface name of the primary ISP (should be ``eth0'').

 
 \item ``nornet\_site\_tbp$N$\_index''\index{nornet\_site\_tbp$N$\_index}: The provider index of the $N$-th ISP (see Chapter~\ref{cha:Providers}).
 
 \item ``nornet\_site\_tbp$N$\_interface''\index{nornet\_site\_tbp$N$\_interface}: The interface name of the $N$-th ISP (usually: ``eth$N+1$').

 \item ``nornet\_site\_tbp$N$\_address\_ipv4''\index{nornet\_site\_tbp$N$\_address\_ipv4}: The IPv4 network (i.e.\ address/prefix) of the $N$-th ISP. ``0.0.0.0/0'' means no IPv4 support.
 
 \item ``nornet\_site\_tbp$N$\_gateway\_ipv4''\index{nornet\_site\_tbp$N$\_gateway\_ipv4}: The IPv4 gateway of the $N$-th ISP; ``0.0.0.0'' in case of no IPv4 support.
 
 \item ``nornet\_site\_tbp$N$\_address\_ipv6''\index{nornet\_site\_tbp$N$\_address\_ipv6}: The IPv6 network (i.e.\ address/prefix) of the $N$-th ISP. ``::/0'' means no IPv6 support.
 
 \item ``nornet\_site\_tbp$N$\_gateway\_ipv6''\index{nornet\_site\_tbp$N$\_gateway\_ipv6}: The IPv6 gateway of the $N$-th ISP; ``::'' in case of no IPv6 support.
 
 \item ``nornet\_site\_tbp$N$\_type''\index{nornet\_site\_tbp$N$\_type}: a description for the interface type of the $N$-th ISP, e.g.\ ``ADSL'', ``Fibre'', ``WLAN'', ``Avian Carrier'', \ldots.

 \item ``nornet\_site\_tbp$N$\_downstream''\index{nornet\_site\_tbp$N$\_downstream}: an estimation for the maximum downstream bandwidth of the $N$-th ISP in Kbit/s (``0'' for undefined).
 
 \item ``nornet\_site\_tbp$N$\_upstream''\index{nornet\_site\_tbp$N$\_upstream}: an estimation for the maximum upstream bandwidth of the $N$-th ISP in Kbit/s (``0'' for undefined).

\end{itemize}
 
An overview of all \noun{NorNet Core} sites can also be found in Chapter~\ref{cha:Sites}, with details on the used ISPs in Chapter~\ref{cha:Providers}.


% ===========================================================================
\subsection{Nodes}
\label{sub:Nodes}
% ===========================================================================

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.90\columnwidth]{%
   Images/PDF/Screenshot-PLC-Nodes.pdf}
\end{center}
\caption{Nodes}
\label{cap:PLC-Nodes}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The ``Nodes''\index{Nodes} section, as depicted in Figure~\ref{cap:PLC-Nodes}, presents a list of all research nodes in the testbed. The search field allows for filtering, e.g.\ by entering parts of the DNS names. That is, using e.g.\ ``.uia.nornet'' will filter for all nodes at the .uia.nornet site at \foreignlanguage{norsk}{Universitetet i Agder\index{Universitetet i Agder}~(UiA)}.

Particularly important is the Status column of the nodes table. It displays the status of each research node, according to its communication with the PLC. The following status values are important:
\begin{itemize}
 \item ``boot''\index{boot}: The node is booted and should be up and running. This is, hopefully, the status of all nodes in the list.
 \item ``failboot''\index{failboot}: Booting has failed for some reason. The node is not usable and the \noun{NorNet Core} staff should check the node's status.
 \item ``reinstall''\index{reinstall}: The node is most likely unusable. When it boots the next time, it will run a complete reinstall. This is also the status of newly added nodes.
\end{itemize}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.90\columnwidth]{%
   Images/PDF/Screenshot-PLC-Node-julenisse.pdf}
\end{center}
\caption{Nodes $\rightarrow$ Node julenisse.uia.nornet}
\label{cap:PLC-Node-Details}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Clicking on a research node displays details about the node itself, as shown in Figure~\ref{cap:PLC-Node-Details} for the node julenisse.uia.nornet\index{julenisse.uia.nornet}. \noun{NorNet Core}-specific configuration settings are stored in custom tags in the PLC database, together with some ``standard'' PLC tags. Some important tags to be noted here are:
\begin{itemize}
 \item ``arch''\index{arch}: The architecture of the system. It is currently always x86\_64 (i386 or other CPU types are not supported).

 \item ``fcdistro''\index{fcdistro}: The variant of \noun{Fedora Core Linux} that runs on the node, e.g.\ ``fc18'' for version~18.

 \item ``pldistro''\index{pldistro}: The software distribution that is installed on the research nodes. Possible values are currently ``lxc'' (stock \noun{PlanetLab}-LXC software) and ``nornet'' (the \noun{NorNet} distribution, to be introduced in Chapter~\ref{cha:Distribution}).
 
 \item ``nornet\_is\_managed\_node'': Denoted whether the research node is a \noun{NorNet Core}-managed node~(1) or not~(0). \noun{NorNet Core}-managed nodes are assumed to be modified by scripts on the PLC server (by the \noun{NorNet Core} staff). Therefore, configuration of the nodes should \emph{not} be changed in the PLC web interface (even if the user's role allows to do this) to avoid misconfiguration!
\end{itemize}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{center}
\includegraphics[width=0.90\columnwidth]{%
   Images/PDF/Screenshot-PLC-Node-julenisse-Interface.pdf}
\end{center}
\caption{Nodes $\rightarrow$ Node julenisse.uia.nornet $\rightarrow$ Interface~10.1.7.100}
\label{cap:PLC-Node-Interface-Details}
\end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The node details page also contains the list of network interfaces. \noun{NorNet Core} nodes usually have only one interface, configured with multiple logical networks (i.e.\ one for each ISP) as well as IPv4 and IPv6. Clicking on this interface will present the interface details, as shown in Figure~\ref{PLC-Node-Interface-Details} for the interface 10.1.10.104 of the research node julenisse.uia.nornet\index{julenisse.uia.nornet}.

The current version of the PLC web interface supports neither multi-homing nor IPv6. Therefore, only the IPv4 configuration of the primary ISP is found directly in this view. IPv6 configuration as well as the configuration for additional ISPs is completely stored in PLC tags:
\begin{itemize}
 \item ``ipaddr$N$''\index{ipaddr$N$}: The IPv4 adddress of the $N+1$-th ISP. Note, that the IPv4 address of the primary interface is stored directly in the configuration, not in the tags.
 
 \item ``netmask$N$''\index{netmask$N$}: The IPv4 network mask of the $N+1$-th ISP.

 \item ``ipv6addr''\index{ipv6addr}: The IPv6 network address (i.e.\ address/prefix) of the primary ISP.
  
 \item ``ipv6\_defaultgw''\index{ipv6\_defaultgw}: The IPv6 default gateway of the primary ISP.

 \item ``ipv6addr\_secondaries''\index{ipv6addr\_secondaries}: A space-separated list of all other ISPs IPv6 network addresses (i.e.\ address/prefix).
 
 \item ``ipv6\_autoconf''\index{ipv6\_autoconf}: Enables (``on'') or disables (``off'') IPv6 auto-configuration. It must be turned off for \noun{NorNet Core} interfaces.

 \item ``nornet\_ifprovider\_index''\index{nornet\_ifprovider\_index}: The provider index (see Chapter~\ref{cha:Providers}) of the primary ISP.

 \item ``nornet\_is\_managed\_interface'': Denoted whether the interface is a \noun{NorNet Core}-managed interface~(1) or not~(0). \noun{NorNet Core}-managed interfaces are assumed to be modified by scripts on the PLC server (by the \noun{NorNet Core} staff). Therefore, configuration of the interfaces should \emph{not} be changed in the PLC web interface (even if the user's role allows to do this) to avoid misconfiguration!

 \item ``ovs\_bridge''\index{ovs\_bridge}: This is the research nodes's network interface, as provided by \noun{Open vSwitch}\index{Open vSwitch}. It should be ``public0''.

 \item ``ifname''\index{ifname}: This is the physical interface that will be managed by \noun{Open vSwitch}\index{Open vSwitch}. It should be ``eth0''.
\end{itemize}

Details on the research nodes, i.e.\ particularly their IP~addresses for all ISPs, can be found in Chapter~\ref{cha:Site-Details}.


% ===========================================================================
\subsection{Slices and Slivers}
\label{sub:Slices-and-Slivers}
% ===========================================================================

...

Details on the research nodes, i.e.\ particularly their slivers and all IP~addresses, can be found in Chapter~\ref{cha:Site-Details}.



% ###########################################################################
\section{Kontrollsenteret}
\label{sec:Kontrollsenteret}
% ###########################################################################

...



% ###########################################################################
\section{Graphite, etc.}
\label{sec:...}
% ###########################################################################

...



% ###########################################################################
\section{Further Resources}
% ###########################################################################

% ===========================================================================
\subsection{Mailing Lists}
% ===========================================================================

There are currently two mailing lists\index{Mailing Lists} for \noun{NorNet}-related postings:

\begin{itemize}
 \item \noun{NorNet}~Announcements\index{Mailing Lists!\noun{NorNet} Announcements}:
 A list with general announcements about \noun{NorNet}, e.g.\ workshops, conferences, etc..
 The registration to this list can be managed at \url{https://sympa.uio.no/ifi.uio.no/modindex/nornet-announcements}.

 \item \noun{NorNet}~Users\index{Mailing Lists!\noun{NorNet} Users}:
 A list with announcements for \noun{NorNet} users, e.g.\ maintenance, changes of the infrastructure, etc.. It is assumed that all \noun{NorNet} users are subscribed to this list.
 The registration to this list can be managed at \url{https://sympa.uio.no/ifi.uio.no/modindex/nornet-users}.
\end{itemize}


% ===========================================================================
\subsection{Sites Overview}
% ===========================================================================

\url{https://www.nntb.no/pub/nornet-configuration/NorNetCore-Sites.html}

...



% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
\chapter{The Research Nodes}
\label{cha:Research-Nodes}
% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

...

% ###########################################################################
\section{Logging In}
\label{sec:Logging-In}
% ###########################################################################

...

ssh and keys



% ###########################################################################
\section{Installing Software}
\label{sec:Installing-Software}
% ###########################################################################

yum ...

gcc, etc.

...


% ###########################################################################
\section{Using Multi-Homing}
\label{sec:Using-Multi-Homing}
% ###########################################################################

ping and traceroute examples

NetPerfMeter!

...


% ###########################################################################
\section{Using Multi-Path TCP}
\label{sec:Using-Multi-Path-TCP}
% ###########################################################################

the patch

options

...



% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
\chapter{Custom Configuration Possibilities}
\label{cha:Custom-Configuration-Possibilities}
% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

...


% ###########################################################################
\section{Custom Virtual Machines}
\label{sec:Custom-Virtual-Machines}
% ###########################################################################

...


% ###########################################################################
\section{Custom Routing Rules}
\label{sec:Custom-Routing-Rules}
% ###########################################################################

SIP Honeypot project~\cite{IFIPNetworking2014}

...



% ###########################################################################
\section{Summary}
% ###########################################################################

...
